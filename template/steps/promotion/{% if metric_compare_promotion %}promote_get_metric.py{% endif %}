# {% include 'template/license_header' %}


from typing_extensions import Annotated

from zenml import get_step_context, step
from zenml.client import Client
from zenml.logger import get_logger
from zenml.integrations.mlflow.experiment_trackers import MLFlowExperimentTracker

logger = get_logger(__name__)

model_registry = Client().active_stack.model_registry

@step
def promote_get_metric(
    metric: str,
    version: str,
) -> Annotated[float, "metric"]:
    """Get metric for comparison for one model deployment.

    This is an example of a metric calculation step. It get a model deployment
    service and computes metric on recent test dataset.

    This step is parameterized, which allows you to configure the step
    independently of the step code, before running it in a pipeline.
    In this example, the step can be configured to use different input data.
    See the documentation for more information:

        https://docs.zenml.io/user-guide/advanced-guide/configure-steps-pipelines

    Args:
        dataset_tst: The test dataset.
        deployment_service: Model version deployment.

    Returns:
        Metric value for a given deployment on test set.

    """

    ### ADD YOUR OWN CODE HERE - THIS IS JUST AN EXAMPLE ###
    model_version = model_registry.get_model_version(version)
    mlflow_run = mlflow.get_run(run_id=model_version.metadata["mlflow_run_id"])
    logger.info("Getting metric from MLFlow run %s", mlflow_run.info.run_id)

    metric = mlflow_run.data.metrics[metric]
    ### YOUR CODE ENDS HERE ###
    return metric
